{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STA314 Final Project Report - YouTube Spam Comments\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "**Relevance and importance**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Analyses\n",
    "\n",
    "#### Data Cleaning\n",
    "\n",
    "**Stop Words**\n",
    "\n",
    "Stop words are typically the most frequently used words in a language and aare generally considered to have little semantic meaning in the conect of text analysic. These words rare often removed from text data during preprocessing because they can introduce noise without constributing significant information about the content or meaning of the text. Examples in English include \"the\", \"is\", \"in\", \"at\", \"which\", \"and\", etc. Many stop words are function words, which serve a grammatical purpose rather than conveying meaning. They help in structuring sentences but do not provide much insight into the topic or sentiment of the text. \n",
    "\n",
    "By removing the stop words, the size of the vocabulary is reduced. We try to focus on words that carry important information, such as nouns, verbs, adjectives, and adverbs. By reducing the feature space of the model, we decrease the complexity and improve their performance. See the [stop words](Data-Cleaning) section below to see the list of stop words. \n",
    "\n",
    "The process involved in data cleaning for our model: \n",
    "1. First, we converted all the words into lower-case letters.\n",
    "2. We experimented with removal of links, for example words starting with \"http\", \"www\", \"https\". We decided to keep these words as these features deemed to be signficant in model prediction.\n",
    "3. We removed all non-ASCII characters.\n",
    "4. We removed any characters that is not an alphabet, a digit, or a whitespace.\n",
    "5. We removed whitespace characters such as spaces, tabs, and new lines.\n",
    "6. We removed any digit, equivalent to `[0-9]`.\n",
    "7. We removed any trailing whitespace characters from a string either at the beginning or at the end of the sentences. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Feature Selection\n",
    "\n",
    "##### Vectorizing features\n",
    "**TF-IDF vectorizer**\n",
    "Put the math behind tf-idf vectorizer here..\n",
    "\n",
    "**Bag of Words**\n",
    "Explain BOW method..\n",
    "\n",
    "**Word-to-vec vectorzer**\n",
    "Explain word-to-vec model here..\n",
    "\n",
    "**Features**\n",
    "\n",
    "The process described above on data-cleaning, and then converting words into tokens can be performed with a library called `TfidfVectorizer`, with a single line: \\\n",
    "` feature_extraction = TfidfVectorizer(min_df = 2, stop_words = 'english', lowercase = True) `\n",
    "\n",
    "Here, `min_df = 2` removes words that appear in less than 2 sentences. \\\n",
    "`stop_words = english` uses the same process for data cleaning as described in the data cleaning section above.\\\n",
    "This function returns a feature model that can then be used to get tokens from our traning dataset. This process results in a total of 884 features for our training dataset. \n",
    "\n",
    "These parameters were chosen to give us the most simplistic model yet achieving highest performance. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "#### Data Cleaning\n",
    "**Stop Words**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Stop words used: \n",
      " ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Display the stop words used \n",
    "# nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "print(f\" Stop words used: \\n\", stop_words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
